# ğŸ­ Emotion Recognition Model Guide

Quick reference for using the ResNet18-based emotion recognition model.

## ğŸ“‹ Model Info
- **Architecture**: ResNet18 
- **Model File**: `fer2013_resnet_best.pth`
- **Framework**: PyTorch
- **Parameters**: 11.17M

## ğŸ¯ Emotion Classes
The model recognizes 7 emotions:

| Index | Emotion | Color |
|-------|---------|--------|
| 0 | Angry | Red |
| 1 | Disgust | Purple |
| 2 | Fear | Blue |
| 3 | Happy | Green |
| 4 | Sad | Blue |
| 5 | Surprise | Yellow |
| 6 | Neutral | Gray |

## ğŸ“Š Input/Output Format

### Input
- **Shape**: `[N, 1, 48, 48]`
- **Type**: `torch.float32`
- **Range**: `[0.0, 1.0]`
- **Format**: Grayscale 48x48 pixels

### Output
- **Shape**: `[N, 7]`
- **Type**: `torch.float32`
- **Format**: Raw logits (apply softmax for probabilities)

## ğŸ”§ Usage Example

```python
import torch
import torch.nn.functional as F
from PIL import Image
from torchvision import transforms
import cv2

# Load model
from models.resnet import ResNet18
model = ResNet18(num_classes=7)
model.load_state_dict(torch.load('models/fer2013_resnet_best.pth', map_location='cpu'))
model.eval()

# Preprocess function
def preprocess_face(face_rgb):
    """Convert RGB face image to model input tensor"""
    # Convert to grayscale and resize to 48x48
    gray = cv2.cvtColor(face_rgb, cv2.COLOR_RGB2GRAY)
    resized = cv2.resize(gray, (48, 48))
    
    # Convert to tensor and normalize to [0,1]
    tensor = torch.FloatTensor(resized).unsqueeze(0).unsqueeze(0) / 255.0
    return tensor

# Inference function
def predict_emotion(face_rgb):
    """Predict emotion from RGB face image"""
    input_tensor = preprocess_face(face_rgb)
    
    with torch.no_grad():
        logits = model(input_tensor)
        probabilities = F.softmax(logits, dim=1)
        predicted_class = torch.argmax(logits, dim=1).item()
        confidence = probabilities[0][predicted_class].item()
    
    emotions = ['Angry', 'Disgust', 'Fear', 'Happy', 'Sad', 'Surprise', 'Neutral']
    return emotions[predicted_class], confidence

# Example usage
image = cv2.imread('face.jpg')
face_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)
emotion, confidence = predict_emotion(face_rgb)
print(f"Predicted: {emotion} (confidence: {confidence:.3f})")
```

## ğŸš€ Quick Start

1. Load the model from `models/fer2013_resnet_best.pth`
2. Preprocess face images to grayscale 48x48 tensors  
3. Run inference and apply softmax to get probabilities
4. Use argmax to get the predicted emotion class

For more details, run the analysis script:
```bash
python analyze_emotion_model.py
```
```python
# æ ‡å‡†é¢„å¤„ç†ç®¡é“
emotion_transform = transforms.Compose([
    transforms.ToPILImage(),           # numpy -> PIL
    transforms.Grayscale(),            # RGB -> Grayscale
    transforms.Resize((48, 48)),       # ä»»æ„å°ºå¯¸ -> 48x48
    transforms.ToTensor(),             # PIL -> Tensor, [0,255] -> [0,1]
])

# åº”ç”¨å˜æ¢
def apply_transform(face_rgb):
    tensor = emotion_transform(face_rgb)      # [1, 48, 48]
    return tensor.unsqueeze(0)                # [1, 1, 48, 48]
```

## âš™ï¸ æ¨ç†æµç¨‹

### æ ‡å‡†æ¨ç†æ­¥éª¤

```python
import torch
import torch.nn.functional as F
from models.resnet import ResNet18
from utils.emotion_recognition import EMOTIONS

def emotion_inference_pipeline(face_tensor, model_path="models/fer2013_resnet_best.pth"):
    """
    å®Œæ•´çš„æƒ…ç»ªè¯†åˆ«æ¨ç†æµç¨‹
    
    Args:
        face_tensor: é¢„å¤„ç†åçš„äººè„¸å¼ é‡ [1, 1, 48, 48]
        model_path: æ¨¡å‹æ–‡ä»¶è·¯å¾„
    
    Returns:
        dict: è¯¦ç»†çš„æ¨ç†ç»“æœ
    """
    
    # 1. è®¾å¤‡é€‰æ‹©
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    print(f"ä½¿ç”¨è®¾å¤‡: {device}")
    
    # 2. åŠ è½½æ¨¡å‹
    model = ResNet18()
    model.load_state_dict(torch.load(model_path, map_location=device))
    model.to(device)
    model.eval()  # è®¾ç½®ä¸ºè¯„ä¼°æ¨¡å¼
    print("æ¨¡å‹åŠ è½½å®Œæˆ")
    
    # 3. å‡†å¤‡è¾“å…¥
    face_tensor = face_tensor.to(device)
    print(f"è¾“å…¥å¼ é‡å½¢çŠ¶: {face_tensor.shape}")
    print(f"è¾“å…¥è®¾å¤‡: {face_tensor.device}")
    
    # 4. å‰å‘ä¼ æ’­
    with torch.no_grad():  # ç¦ç”¨æ¢¯åº¦è®¡ç®—
        start_time = time.time()
        
        # æ¨¡å‹æ¨ç†
        logits = model(face_tensor)  # [1, 7]
        
        # è®¡ç®—æ¦‚ç‡åˆ†å¸ƒ
        probabilities = F.softmax(logits, dim=1)  # [1, 7]
        
        inference_time = time.time() - start_time
        print(f"æ¨ç†æ—¶é—´: {inference_time*1000:.2f}ms")
    
    # 5. ç»“æœè§£æ
    # è·å–é¢„æµ‹ç±»åˆ«
    predicted_class = torch.argmax(logits, dim=1).item()
    predicted_emotion = EMOTIONS[predicted_class]
    confidence = probabilities[0][predicted_class].item()
    
    # è·å–æ‰€æœ‰ç±»åˆ«æ¦‚ç‡
    all_probs = {
        EMOTIONS[i]: probabilities[0][i].item() 
        for i in range(len(EMOTIONS))
    }
    
    # 6. ç»„ç»‡è¿”å›ç»“æœ
    result = {
        'predicted_class': predicted_class,
        'predicted_emotion': predicted_emotion,
        'confidence': confidence,
        'confidence_percent': f"{confidence*100:.2f}%",
        'all_probabilities': all_probs,
        'raw_logits': logits[0].cpu().numpy().tolist(),
        'inference_time_ms': inference_time * 1000,
        'device_used': str(device)
    }
    
    return result
```

### æ‰¹é‡æ¨ç†
```python
def batch_emotion_inference(face_tensors_list, model_path="models/fer2013_resnet_best.pth"):
    """
    æ‰¹é‡æƒ…ç»ªè¯†åˆ«æ¨ç†
    
    Args:
        face_tensors_list: äººè„¸å¼ é‡åˆ—è¡¨ï¼Œæ¯ä¸ªå…ƒç´ å½¢çŠ¶ä¸º [1, 48, 48]
        model_path: æ¨¡å‹æ–‡ä»¶è·¯å¾„
    
    Returns:
        list: æ¯ä¸ªäººè„¸çš„æ¨ç†ç»“æœåˆ—è¡¨
    """
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    
    # åŠ è½½æ¨¡å‹
    model = ResNet18().to(device)
    model.load_state_dict(torch.load(model_path, map_location=device))
    model.eval()
    
    # åˆå¹¶ä¸ºæ‰¹æ¬¡
    batch_tensor = torch.cat(face_tensors_list, dim=0)  # [N, 1, 48, 48]
    batch_tensor = batch_tensor.to(device)
    
    # æ‰¹é‡æ¨ç†
    with torch.no_grad():
        logits = model(batch_tensor)  # [N, 7]
        probabilities = F.softmax(logits, dim=1)  # [N, 7]
    
    # è§£ææ¯ä¸ªç»“æœ
    results = []
    for i in range(len(face_tensors_list)):
        predicted_class = torch.argmax(logits[i], dim=0).item()
        confidence = probabilities[i][predicted_class].item()
        
        results.append({
            'predicted_emotion': EMOTIONS[predicted_class],
            'confidence': confidence,
            'all_probabilities': {
                EMOTIONS[j]: probabilities[i][j].item() 
                for j in range(len(EMOTIONS))
            }
        })
    
    return results
```

## ğŸ’» å®Œæ•´ä»£ç ç¤ºä¾‹

### å•å¼ å›¾åƒæƒ…ç»ªè¯†åˆ«

```python
import cv2
import torch
import torch.nn.functional as F
import numpy as np
import time
from PIL import Image
from torchvision import transforms

from models.resnet import ResNet18
from utils.emotion_recognition import EMOTIONS

class EmotionRecognizer:
    """æƒ…ç»ªè¯†åˆ«å™¨ç±»"""
    
    def __init__(self, model_path="models/fer2013_resnet_best.pth"):
        """
        åˆå§‹åŒ–æƒ…ç»ªè¯†åˆ«å™¨
        
        Args:
            model_path: æƒ…ç»ªè¯†åˆ«æ¨¡å‹æ–‡ä»¶è·¯å¾„
        """
        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        self.model = self._load_model(model_path)
        self.transform = self._create_transform()
        
        print(f"æƒ…ç»ªè¯†åˆ«å™¨åˆå§‹åŒ–å®Œæˆï¼Œä½¿ç”¨è®¾å¤‡: {self.device}")
    
    def _load_model(self, model_path):
        """åŠ è½½è®­ç»ƒå¥½çš„æ¨¡å‹"""
        model = ResNet18()
        model.load_state_dict(torch.load(model_path, map_location=self.device))
        model.to(self.device)
        model.eval()
        return model
    
    def _create_transform(self):
        """åˆ›å»ºå›¾åƒé¢„å¤„ç†å˜æ¢"""
        return transforms.Compose([
            transforms.ToPILImage(),
            transforms.Grayscale(),
            transforms.Resize((48, 48)),
            transforms.ToTensor(),
        ])
    
    def preprocess(self, face_image_rgb):
        """
        é¢„å¤„ç†äººè„¸å›¾åƒ
        
        Args:
            face_image_rgb: RGBæ ¼å¼çš„äººè„¸å›¾åƒ [H, W, 3]
        
        Returns:
            torch.Tensor: é¢„å¤„ç†åçš„å¼ é‡ [1, 1, 48, 48]
        """
        if len(face_image_rgb.shape) != 3:
            raise ValueError("è¾“å…¥å¿…é¡»æ˜¯RGBå›¾åƒ")
        
        # åº”ç”¨å˜æ¢
        tensor = self.transform(face_image_rgb)  # [1, 48, 48]
        tensor = tensor.unsqueeze(0)             # [1, 1, 48, 48]
        
        return tensor.to(self.device)
    
    def predict(self, face_image_rgb, return_all_probs=True):
        """
        é¢„æµ‹äººè„¸æƒ…ç»ª
        
        Args:
            face_image_rgb: RGBæ ¼å¼çš„äººè„¸å›¾åƒ
            return_all_probs: æ˜¯å¦è¿”å›æ‰€æœ‰ç±»åˆ«çš„æ¦‚ç‡
        
        Returns:
            dict: é¢„æµ‹ç»“æœ
        """
        # é¢„å¤„ç†
        face_tensor = self.preprocess(face_image_rgb)
        
        # æ¨ç†
        start_time = time.time()
        with torch.no_grad():
            logits = self.model(face_tensor)
            probabilities = F.softmax(logits, dim=1)
        inference_time = time.time() - start_time
        
        # è§£æç»“æœ
        predicted_class = torch.argmax(logits, dim=1).item()
        predicted_emotion = EMOTIONS[predicted_class]
        confidence = probabilities[0][predicted_class].item()
        
        result = {
            'emotion': predicted_emotion,
            'class_index': predicted_class,
            'confidence': confidence,
            'confidence_percent': f"{confidence*100:.1f}%",
            'inference_time_ms': inference_time * 1000
        }
        
        if return_all_probs:
            result['all_probabilities'] = {
                EMOTIONS[i]: probabilities[0][i].item()
                for i in range(len(EMOTIONS))
            }
        
        return result
    
    def predict_from_file(self, image_path):
        """ä»å›¾åƒæ–‡ä»¶é¢„æµ‹æƒ…ç»ª"""
        image_bgr = cv2.imread(image_path)
        if image_bgr is None:
            raise FileNotFoundError(f"æ— æ³•è¯»å–å›¾åƒæ–‡ä»¶: {image_path}")
        
        face_rgb = cv2.cvtColor(image_bgr, cv2.COLOR_BGR2RGB)
        return self.predict(face_rgb)

# ä½¿ç”¨ç¤ºä¾‹
def main():
    # åˆå§‹åŒ–æƒ…ç»ªè¯†åˆ«å™¨
    recognizer = EmotionRecognizer()
    
    # æ–¹æ³•1: ä»æ–‡ä»¶é¢„æµ‹
    try:
        result = recognizer.predict_from_file("face_image.jpg")
        print(f"é¢„æµ‹æƒ…ç»ª: {result['emotion']}")
        print(f"ç½®ä¿¡åº¦: {result['confidence_percent']}")
        print(f"æ¨ç†æ—¶é—´: {result['inference_time_ms']:.2f}ms")
        
        if 'all_probabilities' in result:
            print("\næ‰€æœ‰æƒ…ç»ªæ¦‚ç‡:")
            for emotion, prob in result['all_probabilities'].items():
                print(f"  {emotion}: {prob:.4f}")
    
    except FileNotFoundError as e:
        print(f"æ–‡ä»¶é”™è¯¯: {e}")
    
    # æ–¹æ³•2: ä»æ‘„åƒå¤´é¢„æµ‹
    cap = cv2.VideoCapture(0)
    if cap.isOpened():
        print("\næŒ‰ 'q' é€€å‡ºæ‘„åƒå¤´æ¨¡å¼")
        while True:
            ret, frame = cap.read()
            if not ret:
                break
            
            # è½¬æ¢ä¸ºRGB
            face_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
            
            # é¢„æµ‹æƒ…ç»ª
            result = recognizer.predict(face_rgb, return_all_probs=False)
            
            # åœ¨å›¾åƒä¸Šæ˜¾ç¤ºç»“æœ
            emotion_text = f"{result['emotion']}: {result['confidence_percent']}"
            cv2.putText(frame, emotion_text, (10, 30), 
                       cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)
            
            cv2.imshow('Emotion Recognition', frame)
            
            if cv2.waitKey(1) & 0xFF == ord('q'):
                break
        
        cap.release()
        cv2.destroyAllWindows()

if __name__ == "__main__":
    main()
```

### ä¸äººè„¸æ£€æµ‹é›†æˆ

```python
def detect_and_recognize_emotions(image_path, face_detector, emotion_recognizer):
    """
    æ£€æµ‹äººè„¸å¹¶è¯†åˆ«æƒ…ç»ªçš„å®Œæ•´æµç¨‹
    
    Args:
        image_path: è¾“å…¥å›¾åƒè·¯å¾„
        face_detector: äººè„¸æ£€æµ‹å™¨å®ä¾‹
        emotion_recognizer: æƒ…ç»ªè¯†åˆ«å™¨å®ä¾‹
    
    Returns:
        list: æ¯ä¸ªæ£€æµ‹åˆ°çš„äººè„¸åŠå…¶æƒ…ç»ª
    """
    # è¯»å–å›¾åƒ
    image_bgr = cv2.imread(image_path)
    image_rgb = cv2.cvtColor(image_bgr, cv2.COLOR_BGR2RGB)
    
    # æ£€æµ‹äººè„¸
    face_results = face_detector.detect(image_rgb)
    
    # ä¸ºæ¯ä¸ªäººè„¸è¯†åˆ«æƒ…ç»ª
    emotion_results = []
    for i, face_info in enumerate(face_results):
        # æå–äººè„¸åŒºåŸŸ
        x, y, w, h = face_info['bbox']
        face_roi = image_rgb[y:y+h, x:x+w]
        
        # è¯†åˆ«æƒ…ç»ª
        emotion_result = emotion_recognizer.predict(face_roi)
        
        # åˆå¹¶ç»“æœ
        combined_result = {
            'face_id': i,
            'bbox': face_info['bbox'],
            'face_confidence': face_info['confidence'],
            'emotion': emotion_result['emotion'],
            'emotion_confidence': emotion_result['confidence'],
            'emotion_probs': emotion_result.get('all_probabilities', {})
        }
        
        emotion_results.append(combined_result)
    
    return emotion_results

# ä½¿ç”¨ç¤ºä¾‹
results = detect_and_recognize_emotions(
    "group_photo.jpg", 
    face_detector, 
    emotion_recognizer
)

for result in results:
    print(f"äººè„¸ {result['face_id']}: {result['emotion']} "
          f"({result['emotion_confidence']:.2%})")
```

## ğŸš€ æ€§èƒ½ä¼˜åŒ–

### 1. æ¨¡å‹ä¼˜åŒ–

```python
# ä½¿ç”¨TorchScriptä¼˜åŒ–
def optimize_model_torchscript(model, example_input):
    """ä½¿ç”¨TorchScriptä¼˜åŒ–æ¨¡å‹"""
    model.eval()
    traced_model = torch.jit.trace(model, example_input)
    traced_model.save("emotion_model_optimized.pt")
    return traced_model

# ä½¿ç”¨ONNXå¯¼å‡º
def export_to_onnx(model, example_input, output_path="emotion_model.onnx"):
    """å¯¼å‡ºä¸ºONNXæ ¼å¼"""
    model.eval()
    torch.onnx.export(
        model,
        example_input,
        output_path,
        export_params=True,
        opset_version=11,
        input_names=['input'],
        output_names=['output'],
        dynamic_axes={
            'input': {0: 'batch_size'},
            'output': {0: 'batch_size'}
        }
    )
```

### 2. æ¨ç†ä¼˜åŒ–

```python
class OptimizedEmotionRecognizer(EmotionRecognizer):
    """ä¼˜åŒ–ç‰ˆæœ¬çš„æƒ…ç»ªè¯†åˆ«å™¨"""
    
    def __init__(self, model_path, use_half_precision=False):
        super().__init__(model_path)
        
        # åŠç²¾åº¦ä¼˜åŒ–
        if use_half_precision and self.device.type == 'cuda':
            self.model.half()
            self.use_half = True
        else:
            self.use_half = False
    
    def predict_batch(self, face_images_rgb, batch_size=32):
        """æ‰¹é‡é¢„æµ‹ä¼˜åŒ–"""
        results = []
        
        for i in range(0, len(face_images_rgb), batch_size):
            batch_images = face_images_rgb[i:i+batch_size]
            
            # é¢„å¤„ç†æ‰¹æ¬¡
            batch_tensors = []
            for img in batch_images:
                tensor = self.preprocess(img)
                if self.use_half:
                    tensor = tensor.half()
                batch_tensors.append(tensor)
            
            # åˆå¹¶æ‰¹æ¬¡
            batch_input = torch.cat(batch_tensors, dim=0)
            
            # æ‰¹é‡æ¨ç†
            with torch.no_grad():
                logits = self.model(batch_input)
                probabilities = F.softmax(logits, dim=1)
            
            # è§£ææ‰¹æ¬¡ç»“æœ
            for j in range(len(batch_images)):
                predicted_class = torch.argmax(logits[j]).item()
                confidence = probabilities[j][predicted_class].item()
                
                results.append({
                    'emotion': EMOTIONS[predicted_class],
                    'confidence': confidence
                })
        
        return results
```

### 3. å†…å­˜ä¼˜åŒ–

```python
def memory_efficient_processing(video_path, emotion_recognizer, max_frames=100):
    """å†…å­˜é«˜æ•ˆçš„è§†é¢‘å¤„ç†"""
    cap = cv2.VideoCapture(video_path)
    frame_count = 0
    results = []
    
    while cap.isOpened() and frame_count < max_frames:
        ret, frame = cap.read()
        if not ret:
            break
        
        # è½¬æ¢é¢œè‰²ç©ºé—´
        frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
        
        # é¢„æµ‹æƒ…ç»ª
        result = emotion_recognizer.predict(frame_rgb, return_all_probs=False)
        results.append(result)
        
        frame_count += 1
        
        # å®šæœŸæ¸…ç†GPUå†…å­˜
        if frame_count % 50 == 0 and torch.cuda.is_available():
            torch.cuda.empty_cache()
    
    cap.release()
    return results
```

## â“ å¸¸è§é—®é¢˜

### Q1: ä¸ºä»€ä¹ˆéœ€è¦è½¬æ¢ä¸ºç°åº¦å›¾ï¼Ÿ
**åŸå› **: 
- FER2013æ•°æ®é›†ä½¿ç”¨ç°åº¦å›¾åƒè®­ç»ƒ
- æƒ…ç»ªä¸»è¦é€šè¿‡é¢éƒ¨è¡¨æƒ…ä¼ è¾¾ï¼Œé¢œè‰²ä¿¡æ¯ä¸æ˜¯å…³é”®å› ç´ 
- ç°åº¦å›¾å‡å°‘äº†è®¡ç®—å¤æ‚åº¦

**æ³¨æ„**: å¦‚æœè¾“å…¥å½©è‰²å›¾åƒï¼Œæ¨¡å‹æ€§èƒ½å¯èƒ½ä¸‹é™ã€‚

### Q2: è¾“å…¥å›¾åƒå°ºå¯¸å¿…é¡»æ˜¯48Ã—48å—ï¼Ÿ
**æ˜¯çš„**ï¼Œæ¨¡å‹æ¶æ„å›ºå®šäº†è¾“å…¥å°ºå¯¸ã€‚

**è§£å†³æ–¹æ¡ˆ**:
```python
# æ­£ç¡®çš„å°ºå¯¸è°ƒæ•´
resized_image = cv2.resize(face_image, (48, 48))

# é”™è¯¯ï¼šä¸è¦æ”¹å˜è¿™ä¸ªå°ºå¯¸
# resized_image = cv2.resize(face_image, (64, 64))  # ä¼šå¯¼è‡´é”™è¯¯
```

### Q3: å¦‚ä½•å¤„ç†å¤šå¼ äººè„¸ï¼Ÿ
**æ–¹æ³•1**: é€ä¸ªå¤„ç†
```python
for face_bbox in face_detections:
    x, y, w, h = face_bbox
    face_roi = image[y:y+h, x:x+w]
    emotion = recognizer.predict(face_roi)
```

**æ–¹æ³•2**: æ‰¹é‡å¤„ç†ï¼ˆæ¨èï¼‰
```python
face_rois = [image[y:y+h, x:x+w] for x, y, w, h in face_bboxes]
emotions = recognizer.predict_batch(face_rois)
```

### Q4: ç½®ä¿¡åº¦å¾ˆä½æ€ä¹ˆåŠï¼Ÿ
**å¯èƒ½åŸå› **:
- äººè„¸è´¨é‡å·®ï¼ˆæ¨¡ç³Šã€é®æŒ¡ã€å…‰ç…§ä¸ä½³ï¼‰
- è¡¨æƒ…ä¸æ˜æ˜¾
- äººè„¸è§’åº¦è¿‡å¤§

**æ”¹å–„æ–¹æ³•**:
```python
# æ·»åŠ ç½®ä¿¡åº¦é˜ˆå€¼
if result['confidence'] < 0.5:
    print("ä½ç½®ä¿¡åº¦é¢„æµ‹ï¼Œå»ºè®®é‡æ–°é‡‡é›†å›¾åƒ")

# æ£€æŸ¥è¾“å…¥è´¨é‡
def check_face_quality(face_image):
    # æ£€æŸ¥å›¾åƒæ¸…æ™°åº¦
    laplacian_var = cv2.Laplacian(face_image, cv2.CV_64F).var()
    if laplacian_var < 100:
        return "å›¾åƒå¯èƒ½æ¨¡ç³Š"
    
    # æ£€æŸ¥äº®åº¦
    mean_brightness = np.mean(face_image)
    if mean_brightness < 50 or mean_brightness > 200:
        return "å…‰ç…§å¯èƒ½ä¸ä½³"
    
    return "è´¨é‡è‰¯å¥½"
```

### Q5: å¦‚ä½•æé«˜è¯†åˆ«å‡†ç¡®ç‡ï¼Ÿ
**æœ€ä½³å®è·µ**:
1. **é«˜è´¨é‡è¾“å…¥**: ç¡®ä¿äººè„¸æ¸…æ™°ã€æ­£é¢ã€å…‰ç…§è‰¯å¥½
2. **é€‚å½“çš„äººè„¸å¤§å°**: äººè„¸åœ¨å›¾åƒä¸­å æ¯”è¦é€‚ä¸­
3. **é¢„å¤„ç†ä¼˜åŒ–**: å¯è€ƒè™‘å¢åŠ å¯¹æ¯”åº¦ã€å»å™ªç­‰é¢„å¤„ç†
4. **å¤šå¸§èåˆ**: å¯¹è§†é¢‘ï¼Œå¯ä»¥å¯¹è¿ç»­å¸§çš„ç»“æœè¿›è¡Œå¹³æ»‘

```python
def smooth_emotion_predictions(emotion_history, window_size=5):
    """å¹³æ»‘è¿ç»­å¸§çš„æƒ…ç»ªé¢„æµ‹"""
    if len(emotion_history) < window_size:
        return emotion_history[-1]
    
    # è®¡ç®—è¿‘æœŸçª—å£å†…å„æƒ…ç»ªçš„å¹³å‡æ¦‚ç‡
    recent_probs = {}
    for emotion in EMOTIONS.values():
        recent_probs[emotion] = np.mean([
            pred['all_probabilities'][emotion] 
            for pred in emotion_history[-window_size:]
        ])
    
    # è¿”å›å¹³å‡æ¦‚ç‡æœ€é«˜çš„æƒ…ç»ª
    smoothed_emotion = max(recent_probs, key=recent_probs.get)
    return {
        'emotion': smoothed_emotion,
        'confidence': recent_probs[smoothed_emotion]
    }
```

## ğŸ¯ æœ€ä½³å®è·µ

### 1. æ•°æ®é¢„å¤„ç†
```python
# æ¨èçš„é¢„å¤„ç†æµç¨‹
def robust_preprocess(face_image_rgb):
    """é²æ£’çš„é¢„å¤„ç†æµç¨‹"""
    # 1. è¾“å…¥éªŒè¯
    if face_image_rgb.shape[2] != 3:
        raise ValueError("è¾“å…¥å¿…é¡»æ˜¯RGBå›¾åƒ")
    
    # 2. å¯é€‰ï¼šå¯¹æ¯”åº¦å¢å¼º
    lab = cv2.cvtColor(face_image_rgb, cv2.COLOR_RGB2LAB)
    lab[:,:,0] = cv2.createCLAHE(clipLimit=2.0).apply(lab[:,:,0])
    enhanced = cv2.cvtColor(lab, cv2.COLOR_LAB2RGB)
    
    # 3. æ ‡å‡†é¢„å¤„ç†
    return preprocess_face(enhanced)
```

### 2. é”™è¯¯å¤„ç†
```python
def safe_emotion_prediction(face_image_rgb, recognizer):
    """å®‰å…¨çš„æƒ…ç»ªé¢„æµ‹"""
    try:
        result = recognizer.predict(face_image_rgb)
        
        # éªŒè¯ç»“æœåˆç†æ€§
        if result['confidence'] < 0.1:
            return {
                'emotion': 'Uncertain',
                'confidence': 0.0,
                'warning': 'ç½®ä¿¡åº¦è¿‡ä½'
            }
        
        return result
        
    except Exception as e:
        return {
            'emotion': 'Error',
            'confidence': 0.0,
            'error': str(e)
        }
```

### 3. æ€§èƒ½ç›‘æ§
```python
class PerformanceMonitor:
    """æ€§èƒ½ç›‘æ§å™¨"""
    
    def __init__(self):
        self.inference_times = []
        self.confidence_scores = []
    
    def record_inference(self, inference_time, confidence):
        self.inference_times.append(inference_time)
        self.confidence_scores.append(confidence)
    
    def get_stats(self):
        if not self.inference_times:
            return {}
        
        return {
            'avg_inference_time': np.mean(self.inference_times),
            'avg_confidence': np.mean(self.confidence_scores),
            'total_predictions': len(self.inference_times),
            'min_confidence': np.min(self.confidence_scores),
            'max_confidence': np.max(self.confidence_scores)
        }
```

## ğŸ“Š æ¨¡å‹åˆ†æç»“æœ

ä½¿ç”¨ `analyze_emotion_model.py` è„šæœ¬å¯ä»¥è·å–è¯¦ç»†çš„æ¨¡å‹åˆ†ææŠ¥å‘Šã€‚è¿è¡Œä»¥ä¸‹å‘½ä»¤ï¼š

```bash
python analyze_emotion_model.py
```

### æ¨¡å‹æ¶æ„è¯¦ç»†åˆ†æ
```
æƒ…ç»ªè¯†åˆ«æ¨¡å‹åˆ†ææŠ¥å‘Š
============================================================

=== æ¨¡å‹æ¶æ„ä¿¡æ¯ ===
æ¨¡å‹ç±»å‹: ResNet18
æ¨¡å‹æ–‡ä»¶: fer2013_resnet_best.pth
æ¡†æ¶: PyTorch
æ€»å‚æ•°æ•°é‡: 11,173,831
å¯è®­ç»ƒå‚æ•°: 11,173,831

=== è¾“å…¥æ ¼å¼è¯¦è§£ ===
è¾“å…¥å¼ é‡å½¢çŠ¶: [Batch_Size, Channels, Height, Width]
å…·ä½“å°ºå¯¸: [N, 1, 48, 48]
- Batch_Size (N): æ‰¹æ¬¡å¤§å°ï¼Œé€šå¸¸ä¸º 1
- Channels: 1 (ç°åº¦å›¾åƒ)
- Height: 48 åƒç´ 
- Width: 48 åƒç´ 
æ•°æ®ç±»å‹: torch.float32
æ•°å€¼èŒƒå›´: [0, 1] (ç»è¿‡ToTensorå½’ä¸€åŒ–)

=== è¾“å‡ºæ ¼å¼è¯¦è§£ ===
è¾“å‡ºå¼ é‡å½¢çŠ¶: [Batch_Size, Num_Classes]
å…·ä½“å°ºå¯¸: [N, 7]
- Batch_Size (N): ä¸è¾“å…¥æ‰¹æ¬¡å¤§å°ç›¸åŒ
- Num_Classes: 7 (ä¸ƒç§æƒ…ç»ªç±»åˆ«)
æ•°æ®ç±»å‹: torch.float32
æ•°å€¼èŒƒå›´: (-âˆ, +âˆ) (logitsï¼Œæœªç»è¿‡softmax)

=== æƒ…ç»ªç±»åˆ«æ˜ å°„ ===
ç´¢å¼• 0: angry
ç´¢å¼• 1: disgust
ç´¢å¼• 2: fear
ç´¢å¼• 3: happy
ç´¢å¼• 4: sad
ç´¢å¼• 5: surprise
ç´¢å¼• 6: neutral
```

### é¢„å¤„ç†æµç¨‹è¯¦ç»†æ¼”ç¤º
```
=== é¢„å¤„ç†æ­¥éª¤æ¼”ç¤º ===
åŸå§‹äººè„¸å›¾åƒå½¢çŠ¶: (64, 64, 3)
åŸå§‹å›¾åƒæ ¼å¼: RGB uint8, åƒç´ å€¼èŒƒå›´ [0, 255]
é¢„å¤„ç†åå¼ é‡å½¢çŠ¶: torch.Size([1, 1, 48, 48])
é¢„å¤„ç†åæ•°æ®ç±»å‹: torch.float32
é¢„å¤„ç†åæ•°å€¼èŒƒå›´: [0.000, 1.000]

=== é¢„å¤„ç†æµç¨‹è¯¦è§£ ===
1. è¾“å…¥: RGBäººè„¸å›¾åƒ (ä»»æ„å°ºå¯¸)
2. è½¬æ¢ä¸ºPILå›¾åƒ
3. è½¬æ¢ä¸ºç°åº¦å›¾ (.convert('L'))
4. è°ƒæ•´å°ºå¯¸åˆ° 48x48 åƒç´ 
5. è½¬æ¢ä¸ºå¼ é‡ (ToTensor): [0,255] -> [0,1]
6. æ·»åŠ æ‰¹æ¬¡ç»´åº¦: [1, 48, 48] -> [1, 1, 48, 48]
```

### æ¨ç†ç»“æœç¤ºä¾‹
```
=== æ¨¡å‹æ¨ç†æ¼”ç¤º ===
åŠ è½½æ¨¡å‹: models/fer2013_resnet_best.pth
æ¨¡å‹åŸå§‹è¾“å‡ºå½¢çŠ¶: torch.Size([1, 7])
æ¨¡å‹åŸå§‹è¾“å‡º (logits): [-1.234  0.567 -0.789  2.123 -0.345  0.912  1.156]

Softmaxæ¦‚ç‡å½¢çŠ¶: torch.Size([1, 7])
å„ç±»åˆ«æ¦‚ç‡:
  angry: 0.0498 (4.98%)
  disgust: 0.2649 (26.49%)
  fear: 0.0731 (7.31%)
  happy: 0.3312 (33.12%)
  sad: 0.1181 (11.81%)
  surprise: 0.3924 (39.24%)
  neutral: 0.4815 (48.15%)

é¢„æµ‹ç»“æœ:
  é¢„æµ‹ç±»åˆ«ç´¢å¼•: 6
  é¢„æµ‹æƒ…ç»ª: neutral
  ç½®ä¿¡åº¦: 0.4815 (48.15%)
```

### åå¤„ç†æµç¨‹è¯¦è§£
```
=== åå¤„ç†æµç¨‹è¯¦è§£ ===
1. æ¨¡å‹è¾“å‡º: logits [1, 7]
2. åº”ç”¨ softmax: F.softmax(logits, dim=1)
3. è·å–æ¦‚ç‡åˆ†å¸ƒ: [1, 7] æ¦‚ç‡å€¼
4. æ‰¾åˆ°æœ€å¤§æ¦‚ç‡ç´¢å¼•: torch.argmax(logits, dim=1)
5. æ˜ å°„åˆ°æƒ…ç»ªåç§°: EMOTIONS[predicted_index]
6. è·å–ç½®ä¿¡åº¦: probabilities[0][predicted_index]
```

### å®Œæ•´æ¨ç†ä»£ç ç¤ºä¾‹ (æ¥è‡ªåˆ†æè„šæœ¬)
```python
import torch
import torch.nn.functional as F
import cv2
import numpy as np
from models.resnet import ResNet18
from utils.emotion_recognition import preprocess_face, EMOTIONS

def predict_emotion_complete(face_image_rgb, model_path="models/fer2013_resnet_best.pth"):
    """
    å®Œæ•´çš„æƒ…ç»ªè¯†åˆ«æµç¨‹
    
    Args:
        face_image_rgb: RGBæ ¼å¼çš„äººè„¸å›¾åƒ (numpy array)
        model_path: æ¨¡å‹æ–‡ä»¶è·¯å¾„
    
    Returns:
        dict: åŒ…å«é¢„æµ‹ç»“æœçš„å­—å…¸
    """
    # 1. åŠ è½½æ¨¡å‹
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    model = ResNet18().to(device)
    model.load_state_dict(torch.load(model_path, map_location=device))
    model.eval()
    
    # 2. é¢„å¤„ç†
    # è¾“å…¥: RGBå›¾åƒ [H, W, 3] uint8 [0, 255]
    face_tensor = preprocess_face(face_image_rgb)  # -> [1, 1, 48, 48] float32 [0, 1]
    face_tensor = face_tensor.to(device)
    
    # 3. æ¨ç†
    with torch.no_grad():
        logits = model(face_tensor)  # -> [1, 7] float32
        probabilities = F.softmax(logits, dim=1)  # -> [1, 7] float32 [0, 1]
    
    # 4. è§£æç»“æœ
    predicted_class = torch.argmax(logits, dim=1).item()  # int
    predicted_emotion = EMOTIONS[predicted_class]  # str
    confidence = probabilities[0][predicted_class].item()  # float
    
    # 5. è¿”å›è¯¦ç»†ç»“æœ
    result = {
        'predicted_class': predicted_class,
        'predicted_emotion': predicted_emotion,
        'confidence': confidence,
        'all_probabilities': {
            EMOTIONS[i]: probabilities[0][i].item() 
            for i in range(len(EMOTIONS))
        },
        'raw_logits': logits[0].cpu().numpy()
    }
    
    return result

# ä½¿ç”¨ç¤ºä¾‹
if __name__ == "__main__":
    # ä»å›¾åƒæ–‡ä»¶è¯»å–äººè„¸
    image_bgr = cv2.imread("face_image.jpg")
    face_rgb = cv2.cvtColor(image_bgr, cv2.COLOR_BGR2RGB)
    
    # è¿›è¡Œæƒ…ç»ªè¯†åˆ«
    result = predict_emotion_complete(face_rgb)
    
    print(f"é¢„æµ‹æƒ…ç»ª: {result['predicted_emotion']}")
    print(f"ç½®ä¿¡åº¦: {result['confidence']:.4f}")
    print("æ‰€æœ‰æƒ…ç»ªæ¦‚ç‡:")
    for emotion, prob in result['all_probabilities'].items():
        print(f"  {emotion}: {prob:.4f}")
```

### è¿è¡Œæ¨¡å‹åˆ†æ
è¦è·å–å®Œæ•´çš„æ¨¡å‹åˆ†ææŠ¥å‘Šï¼Œè¿è¡Œï¼š
```bash
python analyze_emotion_model.py
```

è¯¥è„šæœ¬å°†æä¾›ï¼š
- è¯¦ç»†çš„æ¨¡å‹æ¶æ„ä¿¡æ¯
- è¾“å…¥è¾“å‡ºæ ¼å¼è§„æ ¼
- é¢„å¤„ç†å’Œåå¤„ç†æµç¨‹æ¼”ç¤º  
- å®Œæ•´çš„æ¨ç†ç¤ºä¾‹ä»£ç 
- å®é™…çš„æ¨¡å‹æ¨ç†ç»“æœ

---

**æœ€åæ›´æ–°**: 2025å¹´5æœˆ24æ—¥  
**ç‰ˆæœ¬**: 1.0.0  
**ç»´æŠ¤è€…**: CV-project-new å›¢é˜Ÿ
